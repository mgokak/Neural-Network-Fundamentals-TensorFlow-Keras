{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clJhQgchuaQc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EKPt7FiFwaDT"
      },
      "outputs": [],
      "source": [
        "# Auto Differentiation Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NONaMgODwcqy"
      },
      "outputs": [],
      "source": [
        "# Declare a tensorflow Variable\n",
        "\n",
        "x = tf.Variable(2.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vKfIp6BVwemE"
      },
      "outputs": [],
      "source": [
        "with tf.GradientTape() as tape:\n",
        "    # perform squaring operation\n",
        "    y = x ** 2\n",
        "\n",
        "    # Now that GradientTape has recorded the operation\n",
        "    # we can calculate the gradient of the operation i.e. dy/dx\n",
        "\n",
        "\n",
        "# Note: We are now outside the GradientTape context\n",
        "# Gradient calculations and updates need to be performed\n",
        "# outside the GradientTape context, or these operations will be\n",
        "# recorded on the tape as well, and increased memory usage.\n",
        "\n",
        "\n",
        "dy_dx = tape.gradient(y, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwdKEEefwik4",
        "outputId": "0b11e2e9-f314-4f28-ca01-08812148e8d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.0\n"
          ]
        }
      ],
      "source": [
        "# Gradient value\n",
        "\n",
        "print(dy_dx.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CrBWP0Q9wlUt"
      },
      "outputs": [],
      "source": [
        "# Back Propagation Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6gKFEef8wqQD"
      },
      "outputs": [],
      "source": [
        "# let's first define the constants and variables\n",
        "\n",
        "x1 = tf.constant(1.3, name=\"x1\")\n",
        "x2 = tf.constant(2.1, name=\"x2\")\n",
        "lr = tf.constant(0.1, name=\"learning_rate\")\n",
        "Y  = tf.constant(1.0, name=\"ground_truth\")\n",
        "\n",
        "# ---------\n",
        "w1 = tf.Variable(0.7, name=\"x3\")\n",
        "w2 = tf.Variable(-0.3, name=\"x3\")\n",
        "b  = tf.Variable(1.0, name=\"b\")\n",
        "\n",
        "# Text formatting\n",
        "bold = \"\\033[1m\"\n",
        "end = \"\\033[0m\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mTPeS0z80vkv"
      },
      "outputs": [],
      "source": [
        "# Implementation for equation A1:  x1w1 + x2w2 + b and its derivative\n",
        "\n",
        "def wx_plus_b(x1, x2, w1, w2, b):\n",
        "    return x1*w1 + x2* w2 + b\n",
        "\n",
        "\n",
        "# Derivative of WX + B w.r.t. its input W and B\n",
        "def grad_wx_plus_b(x1, x2):\n",
        "    return x1, x2, tf.constant(1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Bzk6o05306z4"
      },
      "outputs": [],
      "source": [
        "# Implementation for equation A2: Sigmoid and its derivative\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + tf.math.exp(-x))\n",
        "\n",
        "\n",
        "# Derivative of sigmoid w.r.t. its input.\n",
        "def grad_sigmoid(x):\n",
        "    return sigmoid(x) * (1.0 - sigmoid(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fHkyrw7Y0-gP"
      },
      "outputs": [],
      "source": [
        "# Implementation for equation A3: Binary cross-entropy and its derivative\n",
        "\n",
        "def bce_loss(y_hat, y):\n",
        "    loss = -(y * tf.math.log(y_hat)) - ((1 - y) * tf.math.log(1.0 - y_hat))\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Derivative of binary cross-entropy w.r.t. its input.\n",
        "def grad_bce_loss(y_hat, y):\n",
        "    return -(y / y_hat) + ((1.0 - y) / (1.0 - y_hat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cyvsdA691Ii_"
      },
      "outputs": [],
      "source": [
        "def forward(x1, x2, w1, w2, b, Y):\n",
        "    A1 = wx_plus_b(x1, x2, w1, w2, b)\n",
        "    A2 = sigmoid(A1)\n",
        "    A3 = bce_loss(A2, Y)\n",
        "\n",
        "    return_dict = {\n",
        "        \"A1\": A1,\n",
        "        \"A2\": A2,\n",
        "        \"A3\": A3\n",
        "    }\n",
        "    return return_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8k7AAWOE1Lm4"
      },
      "outputs": [],
      "source": [
        "def backward(x1, x2, Y, A1, A2):\n",
        "\n",
        "    # Compute the gradients of A3 w.r.t  A2 i.e dA3/dA2\n",
        "    d_bce_loss = grad_bce_loss(A2, Y)\n",
        "\n",
        "    # Compute the gradients A2 w.r.t A1 i.e dA2/dA1\n",
        "    d_sigmoid = grad_sigmoid(A1)\n",
        "\n",
        "    # Compute the gradients of weighted sum(z) w.r.t weights and bias\n",
        "    # dA1/dw1, dA1/dw2, dA1/b\n",
        "    d_w1, d_w2, d_b = grad_wx_plus_b(x1, x2)\n",
        "\n",
        "    # Using chain rule to find overall gradient of Loss w.r.t weights and bias\n",
        "    w1_grad = d_bce_loss * d_sigmoid * d_w1\n",
        "    w2_grad = d_bce_loss * d_sigmoid * d_w2\n",
        "    b_grad  = d_bce_loss * d_sigmoid * d_b\n",
        "\n",
        "\n",
        "    return_dict = {\n",
        "        \"dA3_dA2\": d_bce_loss,\n",
        "        \"dA2_dA1\": d_sigmoid,\n",
        "        \"dA1_dw1\": d_w1,\n",
        "        \"dA1_dw2\": d_w2,\n",
        "        \"dA1_db\" : d_b,\n",
        "        \"dA3_dw1\": w1_grad,\n",
        "        \"dA3_dw2\": w2_grad,\n",
        "        \"dA3_db\" : b_grad,\n",
        "    }\n",
        "\n",
        "    return return_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMajz_6t1TgF",
        "outputId": "f387aaca-e333-43de-9046-02daec1b1c11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mForward Pass:\u001b[0m\n",
            "\n",
            "\u001b[1mA1:\u001b[0m 1.2799999713897705\n",
            "\u001b[1mA2:\u001b[0m 0.7824497818946838\n",
            "\u001b[1mA3:\u001b[0m 0.24532553553581238 <---\u001b[1m Initial Loss\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Performing forward pass\n",
        "\n",
        "forward_outputs = forward(x1, x2, w1, w2, b, Y)\n",
        "\n",
        "print(f\"{bold}Forward Pass:{end}\\n\")\n",
        "\n",
        "print(f\"{bold}A1:{end} {forward_outputs['A1'].numpy()}\")\n",
        "print(f\"{bold}A2:{end} {forward_outputs['A2'].numpy()}\")\n",
        "print(f\"{bold}A3:{end} {forward_outputs['A3'].numpy()} <---{bold} Initial Loss{end}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IiQ3ZuM1azz",
        "outputId": "5b777447-effa-4ab1-e171-f9c1fd024c4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mBackward Pass: Step 1\u001b[0m\n",
            "\n",
            "\u001b[1mIndividual Derivatives:\u001b[0m\n",
            "\n",
            "\u001b[1mdA3/dA2\u001b[0m = -1.2780373096466064\n",
            "\u001b[1mdA2/dA1\u001b[0m = 0.17022211849689484\n",
            "\u001b[1mdA1/dw1\u001b[0m = 1.2999999523162842\n",
            "\u001b[1mdA1/dw2\u001b[0m = 2.0999999046325684\n",
            "\u001b[1mdA1/db \u001b[0m = 1.0\n",
            "\n",
            "-----------------\n",
            "\n",
            "\u001b[1mGradient of A3 w.r.t. variables:\u001b[0m\n",
            "\n",
            "\u001b[1mdA3/dw1\u001b[0m = -0.28281527757644653\n",
            "\u001b[1mdA3/dw2\u001b[0m = -0.456855446100235\n",
            "\u001b[1mdA3/db \u001b[0m = -0.21755021810531616\n"
          ]
        }
      ],
      "source": [
        "# Performing backward pass\n",
        "\n",
        "A1 = forward_outputs['A1']\n",
        "A2 = forward_outputs['A2']\n",
        "\n",
        "backward_outputs = backward(x1, x2, Y, A1, A2)\n",
        "\n",
        "print(f\"{bold}Backward Pass: Step 1{end}\\n\")\n",
        "print(f\"{bold}Individual Derivatives:{end}\\n\")\n",
        "\n",
        "print(f\"{bold}dA3/dA2{end} = {backward_outputs['dA3_dA2']}\")\n",
        "print(f\"{bold}dA2/dA1{end} = {backward_outputs['dA2_dA1']}\")\n",
        "print(f\"{bold}dA1/dw1{end} = {backward_outputs['dA1_dw1']}\")\n",
        "print(f\"{bold}dA1/dw2{end} = {backward_outputs['dA1_dw2']}\")\n",
        "print(f\"{bold}dA1/db {end} = {backward_outputs['dA1_db']}\")\n",
        "\n",
        "print(\"\\n-----------------\\n\")\n",
        "\n",
        "print(f\"{bold}Gradient of A3 w.r.t. variables:{end}\\n\")\n",
        "\n",
        "print(f\"{bold}dA3/dw1{end} = {backward_outputs['dA3_dw1']}\")\n",
        "print(f\"{bold}dA3/dw2{end} = {backward_outputs['dA3_dw2']}\")\n",
        "print(f\"{bold}dA3/db {end} = {backward_outputs['dA3_db']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9GwSCK9U1dX1"
      },
      "outputs": [],
      "source": [
        "def weight_update(w1, w2, b, dw1, dw2, db, lr):\n",
        "\n",
        "    # w1, w2 and b are objects of tf.Variable class\n",
        "    # They are updated in place\n",
        "\n",
        "    w1.assign_sub(lr * dw1) # w1 = w1 - lr * dw1\n",
        "    w2.assign_sub(lr * dw2)\n",
        "    b.assign_sub(lr * db)\n",
        "\n",
        "    return w1, w2, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYVfq2Py1gER",
        "outputId": "fae20a99-a8b5-4f6c-8695-2cfc5687c45a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mBackward Pass: Step 2\u001b[0m\n",
            "\n",
            "\u001b[1mParameter Updates\u001b[0m\n",
            "\n",
            "\u001b[1mw1\u001b[0m --> \u001b[1mOld:\u001b[0m 0.699999988079071    \u001b[1mNew:\u001b[0m 0.7282814979553223\n",
            "\u001b[1mw2\u001b[0m --> \u001b[1mOld:\u001b[0m -0.30000001192092896 \u001b[1mNew:\u001b[0m -0.25431448221206665\n",
            "\u001b[1mb\u001b[0m  --> \u001b[1mOld:\u001b[0m 1.0                  \u001b[1mNew:\u001b[0m 1.0217549800872803\n"
          ]
        }
      ],
      "source": [
        "w1_grad = backward_outputs[\"dA3_dw1\"]\n",
        "w2_grad = backward_outputs[\"dA3_dw2\"]\n",
        "b_grad  = backward_outputs[\"dA3_db\"]\n",
        "\n",
        "# keeping a copy of old w and b for comparison\n",
        "# as w and b will be updated inplace\n",
        "\n",
        "w1_old = tf.identity(w1, name=\"old_w1\")\n",
        "w2_old = tf.identity(w2, name=\"old_w2\")\n",
        "b_old  = tf.identity(b,  name=\"old_b\")\n",
        "\n",
        "# Perform Weight Update\n",
        "w1_updated, w2_updated, b_updated = weight_update(w1, w2, b, w1_grad, w2_grad, b_grad, lr)\n",
        "\n",
        "print(f\"{bold}Backward Pass: Step 2{end}\\n\")\n",
        "print(f\"{bold}Parameter Updates{end}\\n\")\n",
        "\n",
        "print(f\"{bold}w1{end} --> {bold}Old:{end} {w1_old.numpy():<20} {bold}New:{end} {w1_updated.numpy()}\")\n",
        "print(f\"{bold}w2{end} --> {bold}Old:{end} {w2_old.numpy():<20} {bold}New:{end} {w2_updated.numpy()}\")\n",
        "print(f\"{bold}b{end}  --> {bold}Old:{end} {b_old.numpy():<19}  {bold}New:{end} {b_updated.numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LygLPic_1q1N",
        "outputId": "064bb168-4e8b-49b1-de50-b187c880df30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mChecking New Loss\u001b[0m:\n",
            "\n",
            "\u001b[1mLOSS\u001b[0m --> \u001b[1mOld:\u001b[0m 0.24532553553581238  \u001b[1mNew:\u001b[0m 0.21369412541389465\n"
          ]
        }
      ],
      "source": [
        "# Comparing the old and new loss\n",
        "# New loss\n",
        "\n",
        "new_forward_outputs = forward(x1, x2, w1_updated, w2_updated, b_updated, Y)\n",
        "\n",
        "old_A3 = forward_outputs[\"A3\"]\n",
        "new_A3 = new_forward_outputs[\"A3\"]\n",
        "\n",
        "# We can also pass w1, w2 and b as the objects are being replaced in-place\n",
        "# _, _, new_loss = forward(x1, x2, w1, w2, b, Y)\n",
        "\n",
        "print(f\"{bold}Checking New Loss{end}:\\n\")\n",
        "\n",
        "print(f\"{bold}LOSS{end} --> {bold}Old:{end} {old_A3.numpy():<20} {bold}New:{end} {new_A3.numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VphwJNC_1zzP"
      },
      "outputs": [],
      "source": [
        "# Using GradientTape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5ehGhWwB15EM"
      },
      "outputs": [],
      "source": [
        "# Redefine the constants and variables\n",
        "\n",
        "x1 = tf.constant(1.3, name=\"x1\")\n",
        "x2 = tf.constant(2.1, name=\"x2\")\n",
        "lr = tf.constant(0.1, name=\"learning_rate\")\n",
        "Y  = tf.constant(1.0, name=\"ground_truth\")\n",
        "\n",
        "# ---------\n",
        "w1 = tf.Variable(0.7, name=\"x3\")\n",
        "w2 = tf.Variable(-0.3, name=\"x3\")\n",
        "b  = tf.Variable(1.0, name=\"b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wR3W-aAH15gf"
      },
      "outputs": [],
      "source": [
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    # record operations\n",
        "    A1 = w1 * x1 + w2 * x2 + b\n",
        "    A2 = sigmoid(A1)\n",
        "    A3 = bce_loss(A2, Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzAnbT2s17LH",
        "outputId": "824d2174-a848-4bcb-dd3d-b474c5d2aeea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mForward Pass:\u001b[0m\n",
            "\n",
            "\u001b[1mA1:\u001b[0m 1.2799999713897705\n",
            "\u001b[1mA2:\u001b[0m 0.7824497818946838\n",
            "\u001b[1mA3:\u001b[0m 0.24532553553581238 <---\u001b[1m Initial Loss\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "print(f\"{bold}Forward Pass:{end}\\n\")\n",
        "\n",
        "print(f\"{bold}A1:{end} {A1.numpy()}\")\n",
        "print(f\"{bold}A2:{end} {A2.numpy()}\")\n",
        "print(f\"{bold}A3:{end} {A3.numpy()} <---{bold} Initial Loss{end}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbvlDsx818qz",
        "outputId": "9b339ed5-7ca8-40cf-b076-a4c4892d1a59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mBackward Pass: Step 1\u001b[0m\n",
            "\n",
            "\u001b[1mIndividual Derivatives:\u001b[0m\n",
            "\n",
            "\u001b[1mdA3/dA2\u001b[0m = -1.2780373096466064\n",
            "\u001b[1mdA2/dA1\u001b[0m = 0.17022213339805603\n",
            "\u001b[1mdA1/dw1\u001b[0m = 1.2999999523162842\n",
            "\u001b[1mdA1/dw2\u001b[0m = 2.0999999046325684\n",
            "\u001b[1mdA1/db\u001b[0m =  1.0\n",
            "\n",
            "-----------------\n",
            "\n",
            "\u001b[1mGradient of A3 wrt. variables:\u001b[0m\n",
            "\n",
            "\u001b[1mdA3/dw1\u001b[0m = -0.2828153073787689\n",
            "\u001b[1mdA3/dw2\u001b[0m = -0.4568554759025574\n",
            "\u001b[1mdA3/db\u001b[0m  = -0.21755023300647736\n"
          ]
        }
      ],
      "source": [
        "print(f\"{bold}Backward Pass: Step 1{end}\\n\")\n",
        "print(f\"{bold}Individual Derivatives:{end}\\n\")\n",
        "\n",
        "\n",
        "dA3_dA2 = tape.gradient(A3, A2)\n",
        "\n",
        "dA2_dA1 = tape.gradient(A2, A1)\n",
        "\n",
        "dA1_dw1 = tape.gradient(A1, w1)\n",
        "\n",
        "dA1_dw2 = tape.gradient(A1, w2)\n",
        "\n",
        "dA1_db  = tape.gradient(A1, b)\n",
        "\n",
        "\n",
        "print(f\"{bold}dA3/dA2{end} = {dA3_dA2}\")\n",
        "print(f\"{bold}dA2/dA1{end} = {dA2_dA1}\")\n",
        "print(f\"{bold}dA1/dw1{end} = {dA1_dw1}\")\n",
        "print(f\"{bold}dA1/dw2{end} = {dA1_dw2}\")\n",
        "print(f\"{bold}dA1/db{end} =  {dA1_db}\")\n",
        "\n",
        "print(\"\\n-----------------\\n\")\n",
        "\n",
        "\n",
        "# implementing Chain rule\n",
        "dA3_dw1 = dA3_dA2 * dA2_dA1 * dA1_dw1\n",
        "dA3_dw2 = dA3_dA2 * dA2_dA1 * dA1_dw2\n",
        "dA3_db  = dA3_dA2 * dA2_dA1 * dA1_db\n",
        "\n",
        "print(f\"{bold}Gradient of A3 wrt. variables:{end}\\n\")\n",
        "\n",
        "print(f\"{bold}dA3/dw1{end} = {dA3_dw1}\")\n",
        "print(f\"{bold}dA3/dw2{end} = {dA3_dw2}\")\n",
        "print(f\"{bold}dA3/db{end}  = {dA3_db}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ak4JOtp-2BEp"
      },
      "outputs": [],
      "source": [
        "# Direct implementation of gradient tape\n",
        "# Redefine the constants and variables\n",
        "\n",
        "x1 = tf.constant(1.3, name=\"x1\")\n",
        "x2 = tf.constant(2.1, name=\"x2\")\n",
        "lr = tf.constant(0.1, name=\"learning_rate\")\n",
        "Y  = tf.constant(1.0, name=\"ground_truth\")\n",
        "\n",
        "# ---------\n",
        "w1 = tf.Variable(0.7, name=\"x3\")\n",
        "w2 = tf.Variable(-0.3, name=\"x3\")\n",
        "b  = tf.Variable(1.0, name=\"b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qJ6x3aIl2GK8"
      },
      "outputs": [],
      "source": [
        "def compute(x1, x2, w1, w2, b, Y):\n",
        "\n",
        "    # Notice we have not used persistent=True\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        outputs = forward(x1, x2, w1, w2, b, Y)\n",
        "\n",
        "    # passing all variables with respect to which\n",
        "    # we want to calculate the derivative of A3\n",
        "    grads = tape.gradient(outputs[\"A3\"], [w1, w2, b])\n",
        "\n",
        "    return outputs, grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG1Rc8Bz2H86",
        "outputId": "e998d6b8-8eb6-40c7-c742-1a047e33896c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mBackward Pass: Step 1\u001b[0m\n",
            "\n",
            "\u001b[1mDirect Gradient of A3 wrt. variables using GradientTape:\u001b[0m\n",
            "\n",
            "\u001b[1mdA3/dw1\u001b[0m = -0.2828153073787689\n",
            "\u001b[1mdA3/dw2\u001b[0m = -0.45685550570487976\n",
            "\u001b[1mdA3/db\u001b[0m  = -0.21755024790763855\n"
          ]
        }
      ],
      "source": [
        "forward_outputs, gradients = compute(x1, x2, w1, w2, b, Y)\n",
        "\n",
        "print(f\"{bold}Backward Pass: Step 1{end}\\n\")\n",
        "print(f\"{bold}Direct Gradient of A3 wrt. variables using GradientTape:{end}\\n\")\n",
        "\n",
        "print(f\"{bold}dA3/dw1{end} = {gradients[0]}\")\n",
        "print(f\"{bold}dA3/dw2{end} = {gradients[1]}\")\n",
        "print(f\"{bold}dA3/db{end}  = {gradients[2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XseAEHwn2KH6",
        "outputId": "1991b96d-f9a5-43d0-e0f4-dd377f87158c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mBackward Pass: Step 2\u001b[0m\n",
            "\n",
            "\u001b[1mParameter Updates\u001b[0m\n",
            "\n",
            "\u001b[1mw1\u001b[0m --> \u001b[1mOld:\u001b[0m 0.699999988079071    \u001b[1mNew:\u001b[0m 0.7282814979553223\n",
            "\u001b[1mw2\u001b[0m --> \u001b[1mOld:\u001b[0m -0.30000001192092896 \u001b[1mNew:\u001b[0m -0.25431445240974426\n",
            "\u001b[1mb\u001b[0m  --> \u001b[1mOld:\u001b[0m 1.0                  \u001b[1mNew:\u001b[0m 1.0217549800872803\n"
          ]
        }
      ],
      "source": [
        "# keeping a copy of old w and b for comparison\n",
        "# as w and b will be updated inplace\n",
        "\n",
        "w1_old = tf.identity(w1, name=\"old_w1\")\n",
        "w2_old = tf.identity(w2, name=\"old_w2\")\n",
        "b_old  = tf.identity(b,  name=\"old_b\")\n",
        "\n",
        "# Perform Weight Update\n",
        "\n",
        "w1_updated, w2_updated, b_updated = weight_update(w1, w2, b, gradients[0], gradients[1], gradients[2], lr)\n",
        "\n",
        "print(f\"{bold}Backward Pass: Step 2{end}\\n\")\n",
        "print(f\"{bold}Parameter Updates{end}\\n\")\n",
        "\n",
        "print(f\"{bold}w1{end} --> {bold}Old:{end} {w1_old.numpy():<20} {bold}New:{end} {w1_updated.numpy()}\")\n",
        "print(f\"{bold}w2{end} --> {bold}Old:{end} {w2_old.numpy():<20} {bold}New:{end} {w2_updated.numpy()}\")\n",
        "print(f\"{bold}b{end}  --> {bold}Old:{end} {b_old.numpy():<19}  {bold}New:{end} {b_updated.numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cEAxuuo2MZd",
        "outputId": "83bec858-c470-476c-c9eb-29b8cd320af6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mChecking New Loss\u001b[0m:\n",
            "\n",
            "\u001b[1mLOSS\u001b[0m --> \u001b[1mOld:\u001b[0m 0.24532553553581238  \u001b[1mNew:\u001b[0m 0.21369412541389465\n"
          ]
        }
      ],
      "source": [
        "# New loss computation\n",
        "\n",
        "new_forward_outputs = forward(x1, x2, w1_updated, w2_updated, b_updated, Y)\n",
        "\n",
        "old_A3 = forward_outputs[\"A3\"]\n",
        "new_A3 = new_forward_outputs[\"A3\"]\n",
        "\n",
        "# We can also pass w1, w2, b due to the objects being replaced in the memory\n",
        "# _, _, new_loss = forward(x1, x2, w1, w2, b, Y)\n",
        "\n",
        "print(f\"{bold}Checking New Loss{end}:\\n\")\n",
        "\n",
        "print(f\"{bold}LOSS{end} --> {bold}Old:{end} {old_A3.numpy():<20} {bold}New:{end} {new_A3.numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZonZhwdA2Pke"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
